from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from app.services.chat_service import ChatService
from app.core.config import CONFIG
import json
from openai import AzureOpenAI
router = APIRouter()
openai_client = AzureOpenAI(
    api_key=CONFIG.AZURE_OPENAI_API_KEY,
    azure_endpoint=CONFIG.AZURE_OPEN_API_ENDPOINT,
    api_version=CONFIG.AZURE_OPENAI_API_VERSION
)

@router.get("/debug")
async def debug_config():
    """
    Debug endpoint to show current configuration values
    """
    return {
        "azure_openai_endpoint": CONFIG.AZURE_OPEN_API_ENDPOINT,
        "azure_openai_deployment": CONFIG.AZURE_OPENAI_DEPLOYMENT,
        "azure_openai_api_version": CONFIG.AZURE_OPENAI_API_VERSION,
        "azure_search_endpoint": CONFIG.AZURE_ENDPOINT,
        "azure_search_index": CONFIG.AZURE_SEARCH_INDEX_NAME,
        "default_openai_model": CONFIG.DEFAULT_OPENAI_MODEL
    }

@router.post("/classify")
async def classify_query(request: Request):
    """
    Test endpoint to classify a query without full RAG processing
    """
    try:
        body = await request.json()
        query = body.get("query", "")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        chat_service = ChatService()
        classified_index = chat_service.classifier.classify_query(query)
        
        return {
            "query": query,
            "classified_index": classified_index,
            "index_mapping": chat_service.classifier.get_index_mapping()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/chat")
async def chat_with_rag(request: Request):
    """
    Chat with RAG - Non-streaming response with automatic index classification
    """
    try:

        # Parse request body manually
        body = await request.json()
        message = body.get("message", "")
        hierarchy_filters = body.get("hierarchy_filters", [])
        index_name = body.get("index_name", None)  # Optional - will be auto-classified if not provided
        limit = body.get("limit", 3)
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        chat_service = ChatService()
        response = await chat_service.chat_with_rag(
            message=message,
            hierarchy_filters=hierarchy_filters,
            index_name=index_name,  # Will be auto-classified if None
            limit=limit
        )
        try:
            openai_response = openai_client.chat.completions.create(
    model="TaxGeniiOpenAi",
    temperature=0,
    messages=[
        {
            "role": "system",
            "content": """
You are given a raw response generated by an LLM.  
Your task is to reformat this response into **clean, professional, and well-structured Markdown**.  

### Formatting Requirements:
1. Use clear and appropriate **headings** and **subheadings**.  
2. Apply **proper spacing** between sections, paragraphs, and lists.  
3. Add multiple line breaks after every image and paragraph and after the headings use <br> for line breaks
4. after every image add a line break use <br> for line breaks
5. after every paragraph add a line break use <br> for line breaks
6. Ensure the final output looks highly **professional and easy to read**.  
7. Return the final result strictly in JSON format as follows:  

{
  "response": "<Your markdown-formatted response>"
}
"""
        },
        {
            "role": "user",
            "content": "LLM Response: \n"+response["response"]
        }
    ],
)
            
            # Clean the OpenAI response before JSON parsing
            raw_content = openai_response.choices[0].message.content
            cleaned_content = raw_content.replace("```markdown", "").replace("```json", "").replace("```", "").replace("---", "")
            
            # Remove control characters that might cause JSON parsing issues
            import re
            cleaned_content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned_content)
            
            try:
                parsed_json = json.loads(cleaned_content)
                response["response"] = parsed_json["response"]
            except json.JSONDecodeError:
                # If JSON parsing fails, use the cleaned content directly
                response["response"] = cleaned_content
                
        except Exception as e:
            print(f"OpenAI formatting error: {e}")
            # If OpenAI formatting fails, return the original response
            pass
        
        # Prepare reference documents for headers
        clean_relevant_documents = []
        if response.get("relevant_documents"):
            clean_relevant_documents = [
                {
                    "title": doc.get("title", ""), 
                    "url": doc.get("url", ""), 
                    "hierarchy": doc.get("hierarchy", "")
                } 
                for doc in response["relevant_documents"]
            ]
        
        # Create response with headers
        from fastapi import Response
        response_obj = Response(
            content=json.dumps(response),
            media_type="application/json",
            headers={
                "X-Reference-Documents": json.dumps(clean_relevant_documents),
                "Access-Control-Expose-Headers": "X-Reference-Documents"
            }
        )
        return response_obj
    except Exception as e:
        print(e)
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/chat/stream")
async def chat_with_rag_streaming(request: Request):
    """
    Chat with RAG - Streaming response with automatic index classification
    """
    try:
        print("chat_with_rag_streaming")
        # Parse request body manually
        body = await request.json()
        message = body.get("message", "")
        hierarchy_filters = body.get("hierarchy_filters", [])
        index_name = body.get("index_name", None)  # Optional - will be auto-classified if not provided
        limit = body.get("limit", 3)
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        chat_service = ChatService()
        
        # Use the chat service to get the response and relevant documents
        response = await chat_service.chat_with_rag(
            message=message,
            hierarchy_filters=hierarchy_filters,
            index_name=index_name,
            limit=limit
        )
        clean_relevant_documents = response.get("relevant_documents", [])
        
        async def generate_stream():
            complete_ans = response.get("response", "")
            # Stream the response in chunks
            chunk_size = 50
            for i in range(0, len(complete_ans), chunk_size):
                chunk = complete_ans[i:i + chunk_size]
                if chunk:
                    yield f"data: {chunk}\n\n"
            # Send completion signal
            yield "data: [DONE]\n\n"
           
        response_headers = {
            
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream",
                "X-Reference-Documents": json.dumps(clean_relevant_documents),
            
            
            'Access-Control-Expose-Headers': 'X-Reference-Documents'
        }
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers=response_headers
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/chat/stream")
async def chat_with_rag_streaming_get(request: Request):
    """
    Chat with RAG - Streaming response via GET (for simple testing) with automatic index classification
    """
    try:
        print("asdsada")
        body = await request.json()
        message = body.get("message", "")
        hierarchy_filters = body.get("hierarchy_filters", [])
        index_name = body.get("index_name", None)  # Optional - will be auto-classified if not provided
        limit = body.get("limit", 3)
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")

        
        # Parse hierarchy filters from comma-separated string
        filters = [f.strip() for f in hierarchy_filters.split(",") if f.strip()] if hierarchy_filters else []
        
        chat_service = ChatService()
        
        async def generate_stream():
            async for chunk in chat_service.chat_with_rag_streaming(
                message=message,
                hierarchy_filters=filters,
                index_name=index_name,  # Will be auto-classified if None
                limit=limit
            ):
                yield f"data: {json.dumps({'chunk': chunk, 'done': False})}\n\n"
            
            yield f"data: {json.dumps({'chunk': '', 'done': True})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
